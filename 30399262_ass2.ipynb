{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">FIT5196 Assignment 2</p>\n",
    "\n",
    "### Student Name : Audi Rivai\n",
    "### Student ID       : 30399262\n",
    "\n",
    "Last Updated: 14 October 2020\n",
    "\n",
    "The content of this assignment are:\n",
    "1. Introduction\n",
    "2. Import Libraries\n",
    "3. Loading Data\n",
    "4. Handle Dirty Data\n",
    "5. Handle Missing Data\n",
    "6. Handle Outliers Data\n",
    "7. Handle Dirty Is expedited delivery columns\n",
    "7. Export Data\n",
    "8. Summary\n",
    "\n",
    "## 1. Introduction\n",
    "This FIT5196 Assignment 2 will demostrate how we clean, check, handle the data such as missing data, dirty data, and outliers data. Using several method we will be explain the step and the reason why we are doing this step:\n",
    "\n",
    "1. Explore the data to detect some errors or anomalies\n",
    "2. Using linear model to detect \n",
    "3. Export the result into `.csv` and files.\n",
    "\n",
    "in the data there are some variable that contain an error and need to be fix which are:\n",
    "\n",
    "* <font color=\"blue\">order_id</font>: A unique id for each order\n",
    "* <font color=\"blue\">customer_id</font>: A unique id for each customer \n",
    "* <font color=\"blue\">date</font>: The date the order was made, given in YYYY-MM-DD format\n",
    "* <font color=\"blue\">nearest_warehouse</font>: A string denoting the name of the nearest warehouse to the customer\n",
    "* <font color=\"blue\">shopping_cart </font>: A list of tuples representing the order items: first element of the tuple is the item ordered, and the second element is the quantity ordered for such item. \n",
    "* <font color=\"blue\">order_price</font>: A float denoting the order price in AUD. The order price is the price of items before any discounts and/or delivery charges are applied.\n",
    "* <font color=\"blue\">customer_lat</font>: Latitude of the customer’s location\n",
    "* <font color=\"blue\">customer_long</font>: Longitude of the customer’s location\n",
    "* <font color=\"blue\">coupon_discount</font>: An integer denoting the percentage discount to be applied to the order_price.\n",
    "* <font color=\"blue\">distance_to_nearest_warehouse</font>: e A float representing the arc distance, in kilometres, between the customer and the nearest warehouse to him/her. (radius of earth: 6378 KM)\n",
    "* <font color=\"blue\">delivery_charges</font>: A float representing the delivery charges of the order\n",
    "* <font color=\"blue\">order_total</font>: A float denoting the total of the order in AUD after all discounts and/or delivery charges are applied.\n",
    "* <font color=\"blue\">season</font>: A string denoting the season in which the order was placed. Refer to this link for details about how seasons are defined. \n",
    "* <font color=\"blue\">is_expedited_delivery</font>: A boolean denoting whether the customer has requested an expedited delivery\n",
    "* <font color=\"blue\">latest_customer_review</font>: A string representing the latest customer review on his/her most recent order\n",
    "* <font color=\"blue\">is_happy_customer</font>: A boolean denoting whether the customer is a happy customer or had an issue with his/her last order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Environment: Python 3.7.0 and Jupyter notebook Libraries used\n",
    "* re (for the regular expression, included in Anaconda Python 3.7)\n",
    "* numpy and pandas for extract the data\n",
    "* SentimentIntensityAnalyzer (to classify the text sentiment)\n",
    "* seaborn and matplotlib.pyplot to visualize data\n",
    "* sklearn linear model to create a model for making prediction\n",
    "* itertools permutations for calculate and get the permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Data\n",
    "\n",
    "In this section there are 3 types of data that we want to solve the first is the dirty data. second is the outliers data and last is the missing data. we will look at the explanation about this data. we will read the csv using pandas `pd.read_csv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>nearest_warehouse</th>\n",
       "      <th>shopping_cart</th>\n",
       "      <th>order_price</th>\n",
       "      <th>delivery_charges</th>\n",
       "      <th>customer_lat</th>\n",
       "      <th>customer_long</th>\n",
       "      <th>coupon_discount</th>\n",
       "      <th>order_total</th>\n",
       "      <th>season</th>\n",
       "      <th>is_expedited_delivery</th>\n",
       "      <th>distance_to_nearest_warehouse</th>\n",
       "      <th>latest_customer_review</th>\n",
       "      <th>is_happy_customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORD033408</td>\n",
       "      <td>ID0579410795</td>\n",
       "      <td>2019-03-30</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>[('iStream', 1), ('pearTV', 2), ('iAssist Line...</td>\n",
       "      <td>14995</td>\n",
       "      <td>76.82</td>\n",
       "      <td>-37.822192</td>\n",
       "      <td>144.968058</td>\n",
       "      <td>15</td>\n",
       "      <td>12822.57</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>True</td>\n",
       "      <td>0.4214</td>\n",
       "      <td>renewal is great the price was perfect and it ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORD406627</td>\n",
       "      <td>ID0355818813</td>\n",
       "      <td>2019-11-10</td>\n",
       "      <td>Bakers</td>\n",
       "      <td>[('iStream', 1), ('pearTV', 1)]</td>\n",
       "      <td>6460</td>\n",
       "      <td>97.49</td>\n",
       "      <td>-37.807381</td>\n",
       "      <td>144.998196</td>\n",
       "      <td>5</td>\n",
       "      <td>6234.49</td>\n",
       "      <td>Spring</td>\n",
       "      <td>True</td>\n",
       "      <td>0.3908</td>\n",
       "      <td>the best best phone ive owned</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORD128696</td>\n",
       "      <td>ID0332546101</td>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>[('Candle Inferno', 2), ('iAssist Line', 2)]</td>\n",
       "      <td>5310</td>\n",
       "      <td>64.11</td>\n",
       "      <td>-37.806092</td>\n",
       "      <td>144.944799</td>\n",
       "      <td>15</td>\n",
       "      <td>4577.61</td>\n",
       "      <td>Winter</td>\n",
       "      <td>False</td>\n",
       "      <td>0.7593</td>\n",
       "      <td>great phone! awesome phone would definitely bu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORD033188</td>\n",
       "      <td>ID4353937734</td>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>Bakers</td>\n",
       "      <td>[('Olivia x460', 1), ('Toshika 750', 1)]</td>\n",
       "      <td>5545</td>\n",
       "      <td>72.59</td>\n",
       "      <td>-37.824925</td>\n",
       "      <td>145.011117</td>\n",
       "      <td>10</td>\n",
       "      <td>5063.09</td>\n",
       "      <td>autumn</td>\n",
       "      <td>False</td>\n",
       "      <td>2.1709</td>\n",
       "      <td>good great phone</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORD398245</td>\n",
       "      <td>ID4297530196</td>\n",
       "      <td>2019-11-29</td>\n",
       "      <td>Bakers</td>\n",
       "      <td>[('Alcon 10', 1), ('Olivia x460', 2)]</td>\n",
       "      <td>11400</td>\n",
       "      <td>106.82</td>\n",
       "      <td>-37.801224</td>\n",
       "      <td>144.983865</td>\n",
       "      <td>10</td>\n",
       "      <td>10366.82</td>\n",
       "      <td>Spring</td>\n",
       "      <td>True</td>\n",
       "      <td>1.3975</td>\n",
       "      <td>great phone until i dropped it great phone unt...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    order_id   customer_id        date nearest_warehouse  \\\n",
       "0  ORD033408  ID0579410795  2019-03-30          Thompson   \n",
       "1  ORD406627  ID0355818813  2019-11-10            Bakers   \n",
       "2  ORD128696  ID0332546101  2019-07-03          Thompson   \n",
       "3  ORD033188  ID4353937734  2019-06-12            Bakers   \n",
       "4  ORD398245  ID4297530196  2019-11-29            Bakers   \n",
       "\n",
       "                                       shopping_cart  order_price  \\\n",
       "0  [('iStream', 1), ('pearTV', 2), ('iAssist Line...        14995   \n",
       "1                    [('iStream', 1), ('pearTV', 1)]         6460   \n",
       "2       [('Candle Inferno', 2), ('iAssist Line', 2)]         5310   \n",
       "3           [('Olivia x460', 1), ('Toshika 750', 1)]         5545   \n",
       "4              [('Alcon 10', 1), ('Olivia x460', 2)]        11400   \n",
       "\n",
       "   delivery_charges  customer_lat  customer_long  coupon_discount  \\\n",
       "0             76.82    -37.822192     144.968058               15   \n",
       "1             97.49    -37.807381     144.998196                5   \n",
       "2             64.11    -37.806092     144.944799               15   \n",
       "3             72.59    -37.824925     145.011117               10   \n",
       "4            106.82    -37.801224     144.983865               10   \n",
       "\n",
       "   order_total  season  is_expedited_delivery  distance_to_nearest_warehouse  \\\n",
       "0     12822.57  Autumn                   True                         0.4214   \n",
       "1      6234.49  Spring                   True                         0.3908   \n",
       "2      4577.61  Winter                  False                         0.7593   \n",
       "3      5063.09  autumn                  False                         2.1709   \n",
       "4     10366.82  Spring                   True                         1.3975   \n",
       "\n",
       "                              latest_customer_review  is_happy_customer  \n",
       "0  renewal is great the price was perfect and it ...               True  \n",
       "1                      the best best phone ive owned               True  \n",
       "2  great phone! awesome phone would definitely bu...               True  \n",
       "3                                   good great phone               True  \n",
       "4  great phone until i dropped it great phone unt...               True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dirty data\n",
    "dirty_df = pd.read_csv(\"30399262_dirty_data.csv\")\n",
    "# list the columns\n",
    "columns_dirty = list(dirty_df.columns)\n",
    "dirty_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see in the dirty data has some anomalies that need to fix we will find the anomalies in very row and count how many error inside the dirty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>nearest_warehouse</th>\n",
       "      <th>shopping_cart</th>\n",
       "      <th>order_price</th>\n",
       "      <th>delivery_charges</th>\n",
       "      <th>customer_lat</th>\n",
       "      <th>customer_long</th>\n",
       "      <th>coupon_discount</th>\n",
       "      <th>order_total</th>\n",
       "      <th>season</th>\n",
       "      <th>is_expedited_delivery</th>\n",
       "      <th>distance_to_nearest_warehouse</th>\n",
       "      <th>latest_customer_review</th>\n",
       "      <th>is_happy_customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORD348366</td>\n",
       "      <td>ID0060088408</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>Nickolson</td>\n",
       "      <td>[('Olivia x460', 2), ('Thunder line', 1)]</td>\n",
       "      <td>4630</td>\n",
       "      <td>156.735</td>\n",
       "      <td>-37.819338</td>\n",
       "      <td>144.983695</td>\n",
       "      <td>10</td>\n",
       "      <td>4323.735</td>\n",
       "      <td>Spring</td>\n",
       "      <td>True</td>\n",
       "      <td>1.2465</td>\n",
       "      <td>five stars love the phone it was flawless</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORD131648</td>\n",
       "      <td>ID3529934922</td>\n",
       "      <td>2019-11-05</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>[('pearTV', 2), ('Thunder line', 2)]</td>\n",
       "      <td>16980</td>\n",
       "      <td>86.600</td>\n",
       "      <td>-37.810039</td>\n",
       "      <td>144.904753</td>\n",
       "      <td>0</td>\n",
       "      <td>17066.600</td>\n",
       "      <td>Spring</td>\n",
       "      <td>False</td>\n",
       "      <td>3.7330</td>\n",
       "      <td>one star junk period</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORD455400</td>\n",
       "      <td>ID6167417948</td>\n",
       "      <td>2019-11-08</td>\n",
       "      <td>Nickolson</td>\n",
       "      <td>[('Thunder line', 2), ('Universe Note', 2)]</td>\n",
       "      <td>11260</td>\n",
       "      <td>84.620</td>\n",
       "      <td>-37.814145</td>\n",
       "      <td>144.963143</td>\n",
       "      <td>15</td>\n",
       "      <td>9655.620</td>\n",
       "      <td>Spring</td>\n",
       "      <td>True</td>\n",
       "      <td>0.7503</td>\n",
       "      <td>wrong phone i ordered a white 128gb phone and ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORD127716</td>\n",
       "      <td>ID2392803738</td>\n",
       "      <td>2019-05-22</td>\n",
       "      <td>Nickolson</td>\n",
       "      <td>[('iAssist Line', 2), ('pearTV', 1)]</td>\n",
       "      <td>10760</td>\n",
       "      <td>33.895</td>\n",
       "      <td>-37.809969</td>\n",
       "      <td>144.978517</td>\n",
       "      <td>0</td>\n",
       "      <td>10793.895</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>False</td>\n",
       "      <td>1.2424</td>\n",
       "      <td>like new i bought this renewed but was very pl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORD315358</td>\n",
       "      <td>ID0589420528</td>\n",
       "      <td>2019-12-25</td>\n",
       "      <td>Nickolson</td>\n",
       "      <td>[('Universe Note', 1), ('Alcon 10', 1), ('Oliv...</td>\n",
       "      <td>17075</td>\n",
       "      <td>74.440</td>\n",
       "      <td>-37.809694</td>\n",
       "      <td>144.972599</td>\n",
       "      <td>5</td>\n",
       "      <td>16295.690</td>\n",
       "      <td>Summer</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0265</td>\n",
       "      <td>bueno like</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    order_id   customer_id        date nearest_warehouse  \\\n",
       "0  ORD348366  ID0060088408  2019-11-12         Nickolson   \n",
       "1  ORD131648  ID3529934922  2019-11-05          Thompson   \n",
       "2  ORD455400  ID6167417948  2019-11-08         Nickolson   \n",
       "3  ORD127716  ID2392803738  2019-05-22         Nickolson   \n",
       "4  ORD315358  ID0589420528  2019-12-25         Nickolson   \n",
       "\n",
       "                                       shopping_cart  order_price  \\\n",
       "0          [('Olivia x460', 2), ('Thunder line', 1)]         4630   \n",
       "1               [('pearTV', 2), ('Thunder line', 2)]        16980   \n",
       "2        [('Thunder line', 2), ('Universe Note', 2)]        11260   \n",
       "3               [('iAssist Line', 2), ('pearTV', 1)]        10760   \n",
       "4  [('Universe Note', 1), ('Alcon 10', 1), ('Oliv...        17075   \n",
       "\n",
       "   delivery_charges  customer_lat  customer_long  coupon_discount  \\\n",
       "0           156.735    -37.819338     144.983695               10   \n",
       "1            86.600    -37.810039     144.904753                0   \n",
       "2            84.620    -37.814145     144.963143               15   \n",
       "3            33.895    -37.809969     144.978517                0   \n",
       "4            74.440    -37.809694     144.972599                5   \n",
       "\n",
       "   order_total  season  is_expedited_delivery  distance_to_nearest_warehouse  \\\n",
       "0     4323.735  Spring                   True                         1.2465   \n",
       "1    17066.600  Spring                  False                         3.7330   \n",
       "2     9655.620  Spring                   True                         0.7503   \n",
       "3    10793.895  Autumn                  False                         1.2424   \n",
       "4    16295.690  Summer                  False                         1.0265   \n",
       "\n",
       "                              latest_customer_review  is_happy_customer  \n",
       "0          five stars love the phone it was flawless               True  \n",
       "1                               one star junk period              False  \n",
       "2  wrong phone i ordered a white 128gb phone and ...              False  \n",
       "3  like new i bought this renewed but was very pl...               True  \n",
       "4                                         bueno like               True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the outlier data\n",
    "outliers_df = pd.read_csv(\"30399262_outlier_data.csv\")\n",
    "outliers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error. based on this data we will look which data is outliers and how we detect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>nearest_warehouse</th>\n",
       "      <th>shopping_cart</th>\n",
       "      <th>order_price</th>\n",
       "      <th>delivery_charges</th>\n",
       "      <th>customer_lat</th>\n",
       "      <th>customer_long</th>\n",
       "      <th>coupon_discount</th>\n",
       "      <th>order_total</th>\n",
       "      <th>season</th>\n",
       "      <th>is_expedited_delivery</th>\n",
       "      <th>distance_to_nearest_warehouse</th>\n",
       "      <th>latest_customer_review</th>\n",
       "      <th>is_happy_customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORD173027</td>\n",
       "      <td>ID5002595025</td>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[('pearTV', 2), ('iAssist Line', 2)]</td>\n",
       "      <td>17070.0</td>\n",
       "      <td>62.72</td>\n",
       "      <td>-37.812548</td>\n",
       "      <td>144.953118</td>\n",
       "      <td>15</td>\n",
       "      <td>14572.22</td>\n",
       "      <td>Winter</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nice good</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORD388411</td>\n",
       "      <td>ID6167229080</td>\n",
       "      <td>2019-03-03</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>[('Lucent 330S', 2), ('Thunder line', 2), ('Al...</td>\n",
       "      <td>24720.0</td>\n",
       "      <td>65.84</td>\n",
       "      <td>-37.821040</td>\n",
       "      <td>144.955238</td>\n",
       "      <td>25</td>\n",
       "      <td>18605.84</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>True</td>\n",
       "      <td>1.1762</td>\n",
       "      <td>i purchased this to download books for school....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORD152479</td>\n",
       "      <td>ID0296027679</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>[('Universe Note', 2), ('iAssist Line', 2), ('...</td>\n",
       "      <td>12580.0</td>\n",
       "      <td>51.97</td>\n",
       "      <td>-37.811275</td>\n",
       "      <td>144.947170</td>\n",
       "      <td>15</td>\n",
       "      <td>10744.97</td>\n",
       "      <td>Summer</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1559</td>\n",
       "      <td>scratches and too old device it has lot of scr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORD452973</td>\n",
       "      <td>ID1560149561</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>[('pearTV', 1), ('Universe Note', 1)]</td>\n",
       "      <td>9760.0</td>\n",
       "      <td>100.36</td>\n",
       "      <td>-37.815400</td>\n",
       "      <td>144.937838</td>\n",
       "      <td>15</td>\n",
       "      <td>8396.36</td>\n",
       "      <td>Spring</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8666</td>\n",
       "      <td>five stars excelent the best cell im front ecu...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORD452188</td>\n",
       "      <td>ID4315295088</td>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>Bakers</td>\n",
       "      <td>[('Thunder line', 2), ('Olivia x460', 1)]</td>\n",
       "      <td>5585.0</td>\n",
       "      <td>79.71</td>\n",
       "      <td>-37.817322</td>\n",
       "      <td>144.991454</td>\n",
       "      <td>10</td>\n",
       "      <td>5106.21</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>True</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>all hail the king listen you get a phone that ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    order_id   customer_id        date nearest_warehouse  \\\n",
       "0  ORD173027  ID5002595025  2019-08-13               NaN   \n",
       "1  ORD388411  ID6167229080  2019-03-03          Thompson   \n",
       "2  ORD152479  ID0296027679  2019-01-05          Thompson   \n",
       "3  ORD452973  ID1560149561  2019-09-27          Thompson   \n",
       "4  ORD452188  ID4315295088  2019-05-31            Bakers   \n",
       "\n",
       "                                       shopping_cart  order_price  \\\n",
       "0               [('pearTV', 2), ('iAssist Line', 2)]      17070.0   \n",
       "1  [('Lucent 330S', 2), ('Thunder line', 2), ('Al...      24720.0   \n",
       "2  [('Universe Note', 2), ('iAssist Line', 2), ('...      12580.0   \n",
       "3              [('pearTV', 1), ('Universe Note', 1)]       9760.0   \n",
       "4          [('Thunder line', 2), ('Olivia x460', 1)]       5585.0   \n",
       "\n",
       "   delivery_charges  customer_lat  customer_long  coupon_discount  \\\n",
       "0             62.72    -37.812548     144.953118               15   \n",
       "1             65.84    -37.821040     144.955238               25   \n",
       "2             51.97    -37.811275     144.947170               15   \n",
       "3            100.36    -37.815400     144.937838               15   \n",
       "4             79.71    -37.817322     144.991454               10   \n",
       "\n",
       "   order_total  season  is_expedited_delivery  distance_to_nearest_warehouse  \\\n",
       "0     14572.22  Winter                  False                            NaN   \n",
       "1     18605.84  Autumn                   True                         1.1762   \n",
       "2     10744.97  Summer                  False                         0.1559   \n",
       "3      8396.36  Spring                   True                         0.8666   \n",
       "4      5106.21  Autumn                   True                         0.8805   \n",
       "\n",
       "                              latest_customer_review  is_happy_customer  \n",
       "0                                          nice good                1.0  \n",
       "1  i purchased this to download books for school....                NaN  \n",
       "2  scratches and too old device it has lot of scr...                0.0  \n",
       "3  five stars excelent the best cell im front ecu...                1.0  \n",
       "4  all hail the king listen you get a phone that ...                1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the missing data\n",
    "missing_df = pd.read_csv(\"30399262_missing_data.csv\")\n",
    "missing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing data is where some value in every colomns is missing and we will fill the data based on the calculation from other model. we will also see the warehouse data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'warehouses.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-736bb25c6b72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read warehouses data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwarehouses_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"warehouses.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# filter latitude and longitude\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwarehouse_lat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwarehouses_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lat\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwarehouse_lon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwarehouses_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lon\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\R\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\R\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\R\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\R\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\R\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'warehouses.csv'"
     ]
    }
   ],
   "source": [
    "# read warehouses data\n",
    "warehouses_df = pd.read_csv(\"warehouses.csv\")\n",
    "# filter latitude and longitude\n",
    "warehouse_lat = warehouses_df[\"lat\"]\n",
    "warehouse_lon = warehouses_df[\"lon\"]\n",
    "# get the name of the warehouse\n",
    "warehouses = warehouses_df.names.tolist()\n",
    "warehouses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that there are three different warehouses which is 'Nickolson', 'Thompson', 'Bakers' we will use this for the dirty data and the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handle Dirty Data\n",
    "\n",
    "In this section we will be looking the process is clean the data. when researcher doing their analysis on their data, some data is not usually clean. Therefore, in order to clean them we will look some of the anomalies in the dirty data and fix them.\n",
    "\n",
    "Fisrt thing is, we create a function for create a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linear model based on the question\n",
    "def linearModel(columns):\n",
    "    # this function generate linear model for delivery charge\n",
    "    # assign predictors\n",
    "    x = columns[['distance_to_nearest_warehouse','is_expedited_delivery',\n",
    "                 'is_happy_customer']]  \n",
    "    # assign the target\n",
    "    y = columns['delivery_charges']\n",
    "    # generate the model\n",
    "    delivery_model = linear_model.LinearRegression()\n",
    "    delivery_model.fit(x, y)\n",
    "    return delivery_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Order_id\n",
    "\n",
    "As you can see we will look at the `order_id` columns we will use `value_counts()` and search whether there is a duplicate or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies Order Id: 0\n"
     ]
    }
   ],
   "source": [
    "# search the order id anomalies that have more than 1 value\n",
    "anom_id = dirty_df.order_id.value_counts() != 1\n",
    "print(\"Anomalies Order Id:\",len(list(anom_id[anom_id].index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no relationship between `order_id` and other variables, therefore there are no anomalies and duplicate `order_id` in the dirty data. We can say that we do not need to fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle customer_id\n",
    "\n",
    "`customer_id` is the id number for customers who make an order. some customer can purchases multiple times. Below we will look if there is a anomalies or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID0777753012', 'ID6167489495', 'ID2385138108', 'ID6167440969', 'ID6167266651', 'ID0541122877', 'ID3208518274', 'ID0580036390', 'ID0492939880']\n"
     ]
    }
   ],
   "source": [
    "# search the customer id anomalies that have more than 1 value\n",
    "multiple_order_cust = dirty_df.customer_id.value_counts() != 1\n",
    "multiple_id_cust = list(multiple_order_cust[multiple_order_cust].index)\n",
    "print(multiple_id_cust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see since the `customer_id` has no relationship with other variables or columns and some people can make multiple order, we can tell that this is not an anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle date\n",
    "\n",
    "we can see from the description the format of the date is `YYYY-MM-DD` the problem of this columns is:\n",
    "\n",
    "* Find the date that have different format\n",
    "* Check the day and month, are they get switch\n",
    "\n",
    "we will check every row and demonstrate how to fix them. we will use `re` packages to print the date pattern and fix them\n",
    "`[0-9]{4}-[0-9]{1,2}-[0-9]{1,2}` the pattern is to get the `YYYY-MM-DD` format. another mistake in the `date` is that the month of the date is in the wrong place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the date as a string type\n",
    "dirty_df['date'] = dirty_df['date'].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list\n",
    "wrong_Date_For = []\n",
    "# change the data into a list\n",
    "date = dirty_df['date'].tolist()\n",
    "\n",
    "# Defines a Regular Expression that depicts a YYYY-MM-DD format.\n",
    "patternName = \"[0-9]{4}-[0-9]{1,2}-[0-9]{1,2}\"\n",
    "\n",
    "# loop for every row to see the pattern\n",
    "for index in range(len(date)):\n",
    "    wrong = re.findall(patternName, dirty_df['date'].iloc[index]) \n",
    "    \n",
    "    # If there is no match, it means the date is incorrectly formatted.\n",
    "    if len(wrong) == 0:\n",
    "        wrong_Date_For.append(dirty_df['date'].iloc[index])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(wrong_Date_For)\n",
    "print(\"incorrect date format: \", len(wrong_Date_For))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empyt list\n",
    "wrong_month_For = []\n",
    "# change the data into a list\n",
    "date2 = dirty_df['date'].tolist()\n",
    "\n",
    "# This is able to find dates in the YYYY-DD-MM format by checking if the number \n",
    "# in the 'DD' field is > 12.\n",
    "for index in range(len(date2)):\n",
    "    if int(date2[index][5:7]) > 12:\n",
    "        wrong_month_For.append(date2[index])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(wrong_month_For)\n",
    "print(\"incorrect month format: \", len(wrong_month_For))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result from above, there are 8 incorrect pattern in the `date` columns and 19 incorrect month location. to fix this problem we will use a loop the get the date in the right place and in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.geeksforgeeks.org/python-program-to-swap-two-elements-in-a-list/\n",
    "\n",
    "# create empyt list\n",
    "clean_date = []\n",
    "\n",
    "# split the date when they have \"-\"\n",
    "for i in dirty_df['date'].str.split(\"-\"):\n",
    "    # if year lenght is not detected switch the value between day and year\n",
    "    if len(i[0]) != 4:\n",
    "        i[0], i[2] = i[2], i[0]\n",
    "    # if month is not detected the switch the value between day and month\n",
    "    if int(i[1]) > 12:\n",
    "        i[1], i[2] = i[2], i[1]\n",
    "    # join them again\n",
    "    i = '-'.join(i)\n",
    "    # append the result \n",
    "    clean_date.append(i)\n",
    "\n",
    "# change the value to the date columns\n",
    "dirty_df['date'] = clean_date\n",
    "# change the date column into a datetime type\n",
    "dirty_df['date']= pd.to_datetime(dirty_df['date'])\n",
    "# extract the month \n",
    "dirty_df['month'] = dirty_df['date'].dt.month\n",
    "# change the date as a string type\n",
    "dirty_df['date'] = dirty_df['date'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this code from above, this will able to fix the date problem. After fix the code we will try to check using the same code from the previous cells and see whether there is another anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_Date_For = []\n",
    "date = dirty_df['date'].tolist()\n",
    "\n",
    "# Defines a Regular Expression that depicts a YYYY-MM-DD format.\n",
    "patternName = \"[0-9]{4}-[0-9]{1,2}-[0-9]{1,2}\"\n",
    "\n",
    "for index in range(len(date)):\n",
    "    wrong = re.findall(patternName, dirty_df['date'].iloc[index]) \n",
    "    \n",
    "    # If there is no match, it means the date is incorrectly formatted.\n",
    "    if len(wrong) == 0:\n",
    "        correct_Date_For.append(dirty_df['date'].iloc[index])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(correct_Date_For)\n",
    "print(\"incorrect date format: \", len(correct_Date_For))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_month_For = []\n",
    "date2 = dirty_df['date'].tolist()\n",
    "\n",
    "# This is able to find dates in the YYYY-DD-MM format by checking if the number \n",
    "# in the 'DD' field is > 12.\n",
    "for index in range(len(date2)):\n",
    "    if int(date2[index][5:7]) > 12:\n",
    "        correct_month_For.append(date2[index])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(correct_month_For)\n",
    "print(\"incorrect month format: \", len(correct_month_For))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_date = len(wrong_month_For) + len(wrong_Date_For)\n",
    "print(\"Total anomalies row in date columns:\", anom_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result there are no `Syntactical Anomalies` in the date problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle season\n",
    "\n",
    "In this part we want to check the anomalies of the data in the `season` columns. The the specific problem is:\n",
    "\n",
    "* Naming problem\n",
    "* is the season match with the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the value count to check the value\n",
    "print(dirty_df[\"season\"].value_counts())\n",
    "\n",
    "# get the anomalies season\n",
    "win_list_index = list(dirty_df[dirty_df[\"season\"] == \"winter\"].index)\n",
    "sum_list_index = list(dirty_df[dirty_df[\"season\"] == \"summer\"].index)\n",
    "spr_list_index = list(dirty_df[dirty_df[\"season\"] == \"spring\"].index)\n",
    "aut_list_index = list(dirty_df[dirty_df[\"season\"] == \"autumn\"].index)\n",
    "\n",
    "name_anomalies_season = win_list_index + sum_list_index +\\\n",
    "    spr_list_index + aut_list_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the naming season in the data\n",
    "dirty_df.season.replace({\"winter\": \"Winter\", \"summer\":\"Summer\",\n",
    "                         \"spring\":\"Spring\", \"autumn\":\"Autumn\"},\n",
    "                        inplace=True)\n",
    "\n",
    "# print the value count and check how many anomalies\n",
    "print(dirty_df[\"season\"].value_counts())\n",
    "print(\"naming season anomalies: \", len(name_anomalies_season))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are 22 anomalies for the naming in the date columns. next we will check the season is match with the month.\n",
    "\n",
    "In Australia, the seasons are defined by grouping the calendar months in the following way:\n",
    "\n",
    "* Spring - the three transition months September, October and November.\n",
    "* Summer - the three hottest months December, January and February.\n",
    "* Autumn - the transition months March, April and May.\n",
    "* Winter - the three coldest months June, July and August.\n",
    "\n",
    "http://www.bom.gov.au/climate/glossary/seasons.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the month in every season\n",
    "summer = [12, 1, 2]\n",
    "autumn = [3, 4, 5]\n",
    "winter = [6, 7, 8]\n",
    "spring = [9, 10, 11]\n",
    "\n",
    "# use group by to count each season and \n",
    "# to check is the season is in the right month\n",
    "check_season = dirty_df.groupby(['season','month']).count()\n",
    "# print the result\n",
    "print(check_season.index)\n",
    "\n",
    "# check how many wrong in each season\n",
    "wrong_summer = list(np.where((dirty_df[\"season\"] == \"Summer\") & \n",
    "                             (~dirty_df['month'].isin(summer)))[0])\n",
    "wrong_autumn = list(np.where((dirty_df[\"season\"] == \"Autumn\") & \n",
    "                             (~dirty_df['month'].isin(autumn)))[0])\n",
    "wrong_winter = list(np.where((dirty_df[\"season\"] == \"Winter\") & \n",
    "                             (~dirty_df['month'].isin(winter)))[0])\n",
    "wrong_spring = list(np.where((dirty_df[\"season\"] == \"Spring\") & \n",
    "                             (~dirty_df['month'].isin(spring)))[0])\n",
    "\n",
    "# Calculate the total mistake in the season\n",
    "wrong_season_month = wrong_summer + wrong_autumn + wrong_winter + wrong_spring\n",
    "print(\"incorrect season based on month:\", len(wrong_season_month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use location that contain the correct month and change the season name\n",
    "dirty_df.loc[dirty_df['month'].isin([12, 1, 2]), \"season\"] = \"Summer\"\n",
    "dirty_df.loc[dirty_df['month'].isin([3, 4, 5]), \"season\"] = \"Autumn\"\n",
    "dirty_df.loc[dirty_df['month'].isin([6, 7, 8]), \"season\"] = \"Winter\"\n",
    "dirty_df.loc[dirty_df['month'].isin([9, 10, 11]), \"season\"] = \"Spring\"\n",
    "\n",
    "# group by to check the result\n",
    "dirty_df.groupby(['season','month']).count().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_season = list(set(name_anomalies_season + wrong_season_month))\n",
    "print(\"total anomalies row in season columns:\" ,len(anom_season))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can tell that there are some mistake in the naming and the month season. for example, autumns suppose to be in the March, April and May, therefore we just need to change the value based on the correct month. some index have both anomalies so we have 27 row anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle customer_lat and customer_long\n",
    "\n",
    "every customer have different coordinate which is the latitude and the longitude. to check the coordinate we will see:\n",
    "\n",
    "* plot the data using package and explore the data\n",
    "* check if the coordinate get switch between latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of the data latitude\n",
    "plt.hist(dirty_df['customer_lat'])\n",
    "plt.title('Histogram of Customer Latitude')\n",
    "plt.xlabel('Customer Latitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many anomalies that have more than 0\n",
    "anom_lat = list(dirty_df[dirty_df['customer_lat']>0]['customer_lat'].index)\n",
    "print(\"incorrect latitude:\", len(anom_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of the data longitude\n",
    "plt.hist(dirty_df['customer_long'])\n",
    "plt.title('Histogram of Customer Longitude')\n",
    "plt.xlabel('Customer Longitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many anomalies that have less than 0\n",
    "anom_long = list(dirty_df[dirty_df['customer_long']<0]['customer_long'].index)\n",
    "print(\"incorrect longitude:\", len(anom_long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the problem for the coordinate is some of the latitude and longitude are get switch. therefore we will get the index and switch them back in order to have the right coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast both these columns into respective lists.\n",
    "custLat = dirty_df['customer_lat'].tolist()\n",
    "custLon = dirty_df['customer_long'].tolist()\n",
    "\n",
    "for index in range(len(custLat)): \n",
    "    # This fixes the second error in which the values are swapped.\n",
    "    if custLat[index] > 0 and custLon[index] < 0:\n",
    "        dirty_df.iloc[index, 7] = custLon[index]\n",
    "        dirty_df.iloc[index, 8] = custLat[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result using histogram to check whether there is a mistake\n",
    "plt.hist(dirty_df['customer_lat'])\n",
    "plt.title('Histogram of Customer Latitude')\n",
    "plt.xlabel('Customer Latitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(dirty_df['customer_long'])\n",
    "plt.title('Histogram of Customer Longitude')\n",
    "plt.xlabel('Customer Longitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"incorrect latitude:\", \n",
    "      len(list(dirty_df[dirty_df['customer_lat']>0]['customer_lat'])))\n",
    "print(\"incorrect longitude:\", \n",
    "      len(list(dirty_df[dirty_df['customer_long']<0]['customer_long'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_coor = list(set(anom_lat + anom_long))\n",
    "print(\"Total Anomalies coordinate:\" , len(anom_coor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the new coordinates is in the correct histogram boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle distance_to_nearest_warehouse and nearest_warehouses\n",
    "\n",
    "as you can see from the `warehouses.csv` there are three type of warehouses, in this section we will look at the nearest warehouse columns and found the anomalies. There are view thing that need to be check:\n",
    "\n",
    "* Naming problem: is there any incorrect naming in the column's value\n",
    "* Check the location and see if the warehouse is in the right place\n",
    "\n",
    "After we check the incorrect naming in the `nearest_warehouses`, however there are some anomalise. this anomalies can be called. `Semantic Anomalies` hinder the data collection from being a comprehensive and non-redundant representation of the mini-world. These types of anomalies include: Integrity constraint violations, contradictions, duplicates and invalid tuples\n",
    "this anomalies also involve in the `distance_to_nearest_warehouse`\n",
    "\n",
    "to find this kind of anomalies in this section we will look at:\n",
    "\n",
    "\n",
    "* using the warehouse data create a distance function, to calculate all the distance and check the distance and warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the value of the nearest warehouse\n",
    "print(dirty_df.nearest_warehouse.value_counts())\n",
    "# filter the incorrect name and calculate how many of them\n",
    "nic_list_index=list(dirty_df[dirty_df[\n",
    "    \"nearest_warehouse\"] == \"nickolson\"].index)\n",
    "tho_list_index=list(dirty_df[dirty_df[\n",
    "    \"nearest_warehouse\"] == \"thompson\"].index)\n",
    "bac_list_index=list(dirty_df[dirty_df[\n",
    "    \"nearest_warehouse\"] == \"bakers\"].index)\n",
    "\n",
    "name_anomalies_wr = nic_list_index + tho_list_index + bac_list_index\n",
    "print(\"naming warehouses anomalies: \", len(name_anomalies_wr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based from the table we can see there are some naming problem and we will fix this and see how many mistake the data have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use replace to change the name\n",
    "dirty_df.nearest_warehouse.replace({\"nickolson\": \"Nickolson\", \n",
    "                                    \"thompson\":\"Thompson\", \n",
    "                                    \"bakers\":\"Bakers\"}, \n",
    "                                   inplace=True)\n",
    "\n",
    "# print the value count to check them\n",
    "print(dirty_df.nearest_warehouse.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This anomali is called `Syntactical Anomalies`, it is describe characteristics concerning the format and values used for representation of the entities. Syntactic anomalies such as: lexical errors, domain format errors, syntactical error and irregularities. to solve this problem we will use `replace()` to change the naming problem.\n",
    "\n",
    "After this we will check the place of the warehouse. however we need to check the coodinate and the distance first in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a coordinates based on the warehouse place\n",
    "Nickolson_coor = warehouses_df.iloc[0][[\"lat\", \"lon\"]]\n",
    "Thompson_coor = warehouses_df.iloc[1][[\"lat\", \"lon\"]]\n",
    "Bakers_coor = warehouses_df.iloc[2][[\"lat\", \"lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33029396/\n",
    "# using-pandas-to-calculate-distance-between-coordinates-from-imported-csv\n",
    "\n",
    "# determine the radius\n",
    "R = 6378\n",
    "\n",
    "# create a function that calculate the coordinates \n",
    "# between the customer and the warehouse\n",
    "def dist_places_warehouse(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    d_lat = np.radians(lat2-lat1)\n",
    "    d_lon = np.radians(lon2-lon1)\n",
    "    r_lat1 = np.radians(lat1)\n",
    "    r_lat2 = np.radians(lat2)\n",
    "    \n",
    "    a = np.sin(d_lat/2.) **2 +\\\n",
    "        np.cos(r_lat1) * np.cos(r_lat2) * np.sin(d_lon/2.)**2\n",
    "    haversine = 2 * R * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    return haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new columns that contain distance \n",
    "# from customer coordinate and warehouses\n",
    "dirty_df[\"distance_to_Nickolson\"] = dirty_df.apply(lambda x : \n",
    "                                                   dist_places_warehouse(\n",
    "                                                    x[\"customer_lat\"], \n",
    "                                                    x[\"customer_long\"], \n",
    "                                                    Nickolson_coor[\"lat\"],\n",
    "                                                    Nickolson_coor[\"lon\"]), \n",
    "                                                   axis=1).round(4)\n",
    "\n",
    "dirty_df[\"distance_to_Thompson\"] = dirty_df.apply(lambda x : \n",
    "                                                  dist_places_warehouse(\n",
    "                                                    x[\"customer_lat\"], \n",
    "                                                    x[\"customer_long\"], \n",
    "                                                    Thompson_coor[\"lat\"],\n",
    "                                                    Thompson_coor[\"lon\"]), \n",
    "                                                  axis=1).round(4)\n",
    "\n",
    "dirty_df[\"distance_to_Bakers\"] = dirty_df.apply(lambda x : \n",
    "                                                dist_places_warehouse(\n",
    "                                                  x[\"customer_lat\"], \n",
    "                                                  x[\"customer_long\"], \n",
    "                                                  Bakers_coor[\"lat\"],\n",
    "                                                  Bakers_coor[\"lon\"]), \n",
    "                                                axis=1).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a empty list for monitoring the index \n",
    "# and the incorrect data\n",
    "incorrect_distance = []\n",
    "index_incorrect_distance = []\n",
    "incorrect_warehouse = []\n",
    "index_incorrect_warehouse = []\n",
    "min_nearest = []\n",
    "correct_warehouse = []\n",
    "\n",
    "# list the warehouse place\n",
    "warehouse_places = list(warehouses_df[\"names\"])\n",
    "\n",
    "# loop every row and calculate the distance then check the value\n",
    "for h in range(len(dirty_df)):\n",
    "    dist_near = round(dirty_df[\"distance_to_nearest_warehouse\"][h],\n",
    "                      4)\n",
    "    wr_place = dirty_df[\"nearest_warehouse\"][h]\n",
    "    list_wr = [dirty_df[\"distance_to_Nickolson\"][h],\n",
    "               dirty_df[\"distance_to_Thompson\"][h],\n",
    "               dirty_df[\"distance_to_Bakers\"][h]]\n",
    "    # find the minimum distance\n",
    "    min_dist_idx = list_wr.index(min(list_wr))\n",
    "    correct_warehouse.append(warehouse_places[min_dist_idx])\n",
    "    min_nearest.append(min(list_wr))\n",
    "    # if the warehouse is not the same append \n",
    "    # the value index and value to the list\n",
    "    if wr_place != warehouse_places[min_dist_idx]:\n",
    "        incorrect_warehouse.append(wr_place)\n",
    "        index_incorrect_warehouse.append(h)\n",
    "    # if the distance warehouse is not the same append\n",
    "    # the value index and value to the list\n",
    "    if dist_near not in list_wr:\n",
    "        incorrect_distance.append(dist_near)\n",
    "        index_incorrect_distance.append(h)\n",
    "\n",
    "        \n",
    "print(\"Incorrect distance:\", len(incorrect_distance),\n",
    "      \"\\nIncorrect nearest warehouses:\", len(incorrect_warehouse))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the code there are some row that have incorrect distance warehouse and the place of the warehouses. to fix the error, we will look at the index and changes the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the correct value\n",
    "list_incorrect_warehouse = [min_nearest[i] for i in \n",
    "                            index_incorrect_distance]\n",
    "list_incorrect_distance = [correct_warehouse[i] for i \n",
    "                           in index_incorrect_warehouse]\n",
    "\n",
    "# put the result into the data based on the index\n",
    "dirty_df.loc[index_incorrect_warehouse,\n",
    "             \"nearest_warehouse\"] = list_incorrect_distance\n",
    "dirty_df.loc[index_incorrect_distance,\n",
    "             \"distance_to_nearest_warehouse\"] = list_incorrect_warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same code to check the value\n",
    "new_correct_distance = []\n",
    "index_new_correct_distance = []\n",
    "new_correct_warehouse = []\n",
    "index_new_correct_warehouse = []\n",
    "min_nearest = []\n",
    "correct_warehouse = []\n",
    "warehouse_places = list(warehouses_df[\"names\"])\n",
    "\n",
    "\n",
    "for h in range(len(dirty_df)):\n",
    "    dist_near = round(dirty_df[\"distance_to_nearest_warehouse\"][h],\n",
    "                      4)\n",
    "    wr_place = dirty_df[\"nearest_warehouse\"][h]\n",
    "    list_wr = [dirty_df[\"distance_to_Nickolson\"][h],\n",
    "               dirty_df[\"distance_to_Thompson\"][h],\n",
    "               dirty_df[\"distance_to_Bakers\"][h]]\n",
    "    min_dist_idx = list_wr.index(min(list_wr))\n",
    "    correct_warehouse.append(warehouse_places[min_dist_idx])\n",
    "    min_nearest.append(min(list_wr))\n",
    "    if wr_place != warehouse_places[min_dist_idx]:\n",
    "        new_correct_warehouse.append(wr_place)\n",
    "        index_new_correct_warehouse.append(h)\n",
    "    if dist_near not in list_wr:\n",
    "        new_correct_distance.append(dist_near)\n",
    "        index_new_correct_distance.append(h)\n",
    "\n",
    "        \n",
    "print(\"Incorrect distance:\", len(new_correct_distance),\n",
    "      \"\\nIncorrect nearest warehouses:\", len(new_correct_warehouse))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once we use the code, we can tell that there is no mistake in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_dist_wr = list(index_incorrect_distance)\n",
    "anom_wr = list(set(name_anomalies_wr + index_incorrect_warehouse))\n",
    "\n",
    "print(\"Total Incorrect distance:\", len(anom_dist_wr),\n",
    "      \"\\nTotal Incorrect nearest warehouses:\", len(anom_wr))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can tell that some of the nearest warehouse have the same index so the number of row that have anomalies in warehouse and the distance are 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle shopping_cart, order_price, order_total\n",
    "\n",
    "in this section we will be looking at the `shopping_cart`, `order_price`, `order_total`. To check the anomalies in each row, we will be using at some of the step which is:\n",
    "\n",
    "* using `np.linalg.solve` to calculate the price of each item in the outliers data\n",
    "* to calculate the price and check them, we need to use the equation of each item price.\n",
    "* calculate the total price using 2 different order prices, one of them from the original, others is the price from the each item price.\n",
    "\n",
    "as you can see, there is no mistake `shopping_cart`, `order_price`, `order_total` in the outliers data. therefore we will use the outliers data to get the each value item price then check every single row in those column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the item and the quantity from the quantity using replace and evaluate\n",
    "outliers_df['item_list'] = outliers_df.shopping_cart\\\n",
    "                .str.replace(\"(\", \"[\").str.replace(\")\", \"]\").apply(eval)\n",
    "# create empty dictionary\n",
    "items_warehouse_dict = {}\n",
    "\n",
    "# create fucntion that calculate the total quantity in each warehouse\n",
    "def check_type(columns):\n",
    "    for tup in columns['item_list']:\n",
    "        if tup[0] not in items_warehouse_dict:\n",
    "            items_warehouse_dict [tup[0]] = {}\n",
    "        if columns['nearest_warehouse'] not in items_warehouse_dict [tup[0]]:\n",
    "            items_warehouse_dict[tup[0]][columns['nearest_warehouse']] = 1\n",
    "        else:\n",
    "            items_warehouse_dict[tup[0]][columns['nearest_warehouse']] += 1\n",
    "\n",
    "# apply the function in every row           \n",
    "outliers_df[['nearest_warehouse','item_list']].apply(check_type,axis=\"columns\")\n",
    "items_warehouse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 10 different item in the data. we will create an array that contain the quantity of the item. the array order based on the item from the dictionary in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list for the warehouse\n",
    "nearest_warehouses = outliers_df.nearest_warehouse.unique().tolist()\n",
    "# list the items\n",
    "items_cart = list(items_warehouse_dict.keys())\n",
    "# extract the item and the quantity from the quantity using replace and evaluate\n",
    "outliers_df['item_list'] = outliers_df.shopping_cart\\\n",
    "                    .str.replace(\"(\", \"[\").str.replace(\")\", \"]\").apply(eval)\n",
    "\n",
    "# create a function that create a numpy array of a quantity in the item \n",
    "# and order them based on the dict key\n",
    "def pro_qty(value):\n",
    "    item_detail = dict(value['item_list'])\n",
    "    quantity = list(item_detail.values())\n",
    "    items_product = list(item_detail.keys())\n",
    "    quantity_list = []\n",
    "    for n in items_cart:\n",
    "        if n in items_product:\n",
    "            quantity_list.append(quantity[items_product.index(n)])\n",
    "        else:\n",
    "            quantity_list.append(0)\n",
    "    return quantity_list\n",
    "\n",
    "# apply the function to get a new columns\n",
    "outliers_df['linalg_arr'] = outliers_df.apply(pro_qty,axis=1)\n",
    "outliers_df[['item_list','linalg_arr']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`linalg_arr` is a column that contain the quantity of the item. we will be using this array to calculate each price of an item using `np.linalg.solve()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_warehouse = outliers_df.nearest_warehouse.unique().tolist()\n",
    "# calculate a price for each item \n",
    "warehouse_price_item = {}\n",
    "\n",
    "# loop for every warehouse\n",
    "for warehouse in nearest_warehouse:\n",
    "    items = list(items_warehouse_dict.keys())\n",
    "    \n",
    "    # take sample based on the number of the item\n",
    "    data = outliers_df[(outliers_df['nearest_warehouse']==warehouse)]\\\n",
    "                    .sample(n=len(items))\n",
    "    \n",
    "    # change the 'linalg_arr' to an array\n",
    "    variables = np.array(data['linalg_arr'].tolist())\n",
    "    \n",
    "    # if the determinant is 0 we will take new sample\n",
    "    while np.linalg.det(variables) == 0:\n",
    "        data = outliers_df[(outliers_df['nearest_warehouse']==warehouse)]\\\n",
    "                    .sample(n=len(items))\n",
    "        variables = np.array(data['linalg_arr'].tolist())\n",
    "        \n",
    "    # use np.linalg.solve() to find the linearity change float and round to 2\n",
    "    price = np.array(data['order_price'].apply(lambda x: float(x)).tolist())\n",
    "    linear_sol = np.linalg.solve(variables,price).round(2)\n",
    "    \n",
    "    # if there is a negative number then resample again\n",
    "    while sum(1 for number in linear_sol if number < 0) != 0:\n",
    "        data = outliers_df[(outliers_df['nearest_warehouse']==warehouse)]\\\n",
    "                    .sample(n=len(items))\n",
    "        variables = np.array(data['linalg_arr'].tolist())\n",
    "        while np.linalg.det(variables) == 0:\n",
    "            data = outliers_df[(outliers_df['nearest_warehouse']==warehouse)]\\\n",
    "                    .sample(n=len(items))\n",
    "            variables = np.array(data['linalg_arr'].tolist())\n",
    "        price = np.array(data['order_price'].apply(lambda x: float(x)).tolist())\n",
    "        linear_sol = np.linalg.solve(variables,price).round(2)\n",
    "        \n",
    "    # change the result to dictionary\n",
    "    c = dict(zip(items, linear_sol))\n",
    "    for i in range(len(items)):\n",
    "        warehouse_price_item[warehouse] = c\n",
    "        \n",
    "print(\"Price item in Nickolson:\", warehouse_price_item['Nickolson'])\n",
    "print(\"Price item in Thompson:\", warehouse_price_item['Thompson'])\n",
    "print(\"Price item in Bakers:\", warehouse_price_item['Bakers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the result, in each warehouse they all have the same price. therefore we will use this price to calculate the price and the total. below there will be 2 new different order total:\n",
    "\n",
    "* `price_total_new`: calulation of the new order price and `coupon_discount` then add with the `delivery_charges`\n",
    "* `price_total_old`: calulation of the original order price and `coupon_discount` then add with the `delivery_charges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dot product between the quantity array with the each price\n",
    "def dot_pro(columns):\n",
    "    price_result = int(np.dot(np.array(columns['linalg_arr']), linear_sol))\n",
    "    return price_result\n",
    "\n",
    "# calculate the total price using the new order price\n",
    "def calculate_total_new(columns):\n",
    "    discount = (columns[\"coupon_discount\"]/100)\n",
    "    total_price_result = (columns[\"price_result_new\"] - \n",
    "                          (columns[\"price_result_new\"]*discount))\n",
    "    total_price_result = total_price_result + columns[\"delivery_charges\"]\n",
    "\n",
    "    return total_price_result\n",
    "\n",
    "# calculate the total price using the original price from the dirty data\n",
    "def calculate_total_old(columns):\n",
    "    discount = (columns[\"coupon_discount\"]/100)\n",
    "    total_price_result = (columns[\"order_price\"] -\n",
    "                          (columns[\"order_price\"]*discount))\n",
    "    total_price_result = total_price_result + columns[\"delivery_charges\"]\n",
    "    \n",
    "    return total_price_result\n",
    "\n",
    "# apply the function for every row to make a new columns\n",
    "outliers_df[\"price_result_new\"] = outliers_df.apply(dot_pro, axis=\"columns\")\n",
    "outliers_df[\"price_total_new\"] = outliers_df.apply(calculate_total_new,\n",
    "                                                   axis=\"columns\")\n",
    "outliers_df[\"price_total_old\"] = outliers_df.apply(calculate_total_old,\n",
    "                                                   axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same code using the dirty data\n",
    "nearest_warehouses = dirty_df.nearest_warehouse.unique().tolist()\n",
    "items_cart = list(items_warehouse_dict.keys())\n",
    "\n",
    "\n",
    "dirty_df['item_list'] = dirty_df.shopping_cart\\\n",
    "    .str.replace(\"(\", \"[\").str.replace(\")\", \"]\").apply(eval)\n",
    "dirty_df['linalg_arr'] = dirty_df.apply(pro_qty,axis=1)\n",
    "dirty_df[['item_list','linalg_arr']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function for every row to make a new columns\n",
    "dirty_df[\"price_result_new\"] = dirty_df.apply(dot_pro, axis=\"columns\")\n",
    "dirty_df[\"price_total_new\"] = dirty_df.apply(calculate_total_new, \n",
    "                                             axis=\"columns\")\n",
    "dirty_df[\"price_total_old\"] = dirty_df.apply(calculate_total_old, \n",
    "                                             axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after we get the new columns, we will detect the anomalies based on this condition:\n",
    "\n",
    "`order_price`: if the new order price is not equal with the original order price and new order total same value with the original order total\n",
    "\n",
    "`order_total`: if the new order price is equal with the original order price and old order total different value with the original order total\n",
    "\n",
    "`shopping_cart`: if the new order price is not equal with the original order price and old order total same value with the original order total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the condition in the code\n",
    "anom_order_price = list(np.where((dirty_df[\"price_result_new\"] != \n",
    "                                  dirty_df['order_price']) &\n",
    "         (dirty_df[\"price_total_new\"] == dirty_df['order_total']))[0])\n",
    "\n",
    "anom_order_total = list(np.where((dirty_df[\"price_result_new\"] == \n",
    "                                  dirty_df['order_price']) &\n",
    "         (dirty_df[\"price_total_old\"] != dirty_df['order_total']))[0])\n",
    "\n",
    "anom_shopping_cart = list(np.where((dirty_df[\"price_result_new\"] != \n",
    "                                    dirty_df['order_price']) &\n",
    "         (dirty_df[\"price_total_old\"] == dirty_df['order_total']))[0])\n",
    "\n",
    "print(\"incorrect order price:\", len(anom_order_price))\n",
    "print(\"incorrect order total:\", len(anom_order_total))\n",
    "print(\"incorrect shopping cart:\", len(anom_shopping_cart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  filter the index and change the order price and order total \n",
    "dirty_df.loc[anom_order_price,\n",
    "             \"order_price\"] = dirty_df.loc[anom_order_price,\n",
    "                                           \"price_result_new\"]\n",
    "dirty_df.loc[anom_order_total,\n",
    "             \"order_total\"] = dirty_df.loc[anom_order_total,\n",
    "                                           \"price_total_new\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are 27 anomalies in every columns. Since the `order_price` and `order_total` are fix. we just need to change the item purchases, since the quantity of the item is correct.\n",
    "\n",
    "to fix the `shopping_cart` we will be using the `permutations()` for every array item's quantity in every columns. After we found the permutations, we will check the value that have the same value with the fix `order_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may take time to process\n",
    "items = list(items_warehouse_dict.keys())\n",
    "\n",
    "# create the function for the permutation\n",
    "def repair_item_cart(columns):\n",
    "    permutate = list(permutations(columns[\"linalg_arr\"]))\n",
    "    \n",
    "    # loop all the permutation to find the right order price\n",
    "    for linear_system in permutate:\n",
    "        check_price = np.dot(np.array(linear_system), linear_sol)\n",
    "        if check_price == columns[\"order_price\"]:\n",
    "            break;\n",
    "            \n",
    "    # clean the result to get the same format\n",
    "    list_item_qty = list(zip(items, list(linear_system)))\n",
    "    result = [(x, y) for x, y in list_item_qty if y != 0]\n",
    "    return str(result)\n",
    "\n",
    "# create fucntion to list the keys item\n",
    "def manipulate_item(column):\n",
    "    return list(dict(eval(column.replace(\"[\", \"\").replace(\"]\", \"\"))).keys())\n",
    "\n",
    "# apply the function in every columns\n",
    "repair_item_cart = dirty_df.loc[anom_shopping_cart].apply(repair_item_cart,\n",
    "                                                          axis = 1)\n",
    "new_value_item = repair_item_cart.apply(manipulate_item)\n",
    "old_value_item = dirty_df.loc[anom_shopping_cart][\"shopping_cart\"]\\\n",
    "                            .apply(manipulate_item)\n",
    "new_item_series = (new_value_item.apply(set) - old_value_item.apply(set))\\\n",
    "                            .apply(list)\n",
    "old_item_series = (old_value_item.apply(set)- new_value_item.apply(set))\\\n",
    "                            .apply(list)\n",
    "\n",
    "# append to list the correct item\n",
    "correction_item = []\n",
    "for c in list(dirty_df.loc[anom_shopping_cart][\"shopping_cart\"].index):\n",
    "    correction_item.append(dirty_df.loc[anom_shopping_cart][\"shopping_cart\"][c]\\\n",
    "                        .replace(old_item_series[c][0],new_item_series[c][0]))\n",
    "\n",
    "# put the result based on the index     \n",
    "dirty_df.loc[anom_shopping_cart, \"shopping_cart\"] = np.array(correction_item)\n",
    "dirty_df['item_list'] = dirty_df.shopping_cart.str.replace(\"(\", \"[\")\\\n",
    "                    .str.replace(\")\", \"]\").apply(eval)\n",
    "dirty_df['linalg_arr'] = dirty_df.apply(pro_qty, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result using the same code\n",
    "dirty_df[\"price_result_new\"] = dirty_df.apply(dot_pro, axis=\"columns\")\n",
    "dirty_df[\"price_total_new\"] = dirty_df.apply(calculate_total_new,\n",
    "                                             axis=\"columns\")\n",
    "dirty_df[\"price_total_old\"] = dirty_df.apply(calculate_total_old,\n",
    "                                             axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result using the same code\n",
    "correct_order_price = list(np.where((dirty_df[\"price_result_new\"] != \n",
    "                                     dirty_df['order_price']) &\n",
    "         (dirty_df[\"price_total_new\"] == dirty_df['order_total']))[0])\n",
    "correct_order_total = list(np.where((dirty_df[\"price_result_new\"] == \n",
    "                                     dirty_df['order_price']) &\n",
    "         (dirty_df[\"price_total_old\"] != dirty_df['order_total']))[0])\n",
    "correct_item_cart = list(np.where((dirty_df[\"price_result_new\"] != \n",
    "                                   dirty_df['order_price']) &\n",
    "         (dirty_df[\"price_total_old\"] == dirty_df['order_total']))[0])\n",
    "\n",
    "print(\"incorrect order price:\", len(correct_order_price))\n",
    "print(\"incorrect order total:\", len(correct_order_total))\n",
    "print(\"incorrect item cart:\", len(correct_item_cart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total anomalies row in order price columns:\", len(anom_order_price))\n",
    "print(\"Total anomalies row in order total columns:\", len(anom_order_total))\n",
    "print(\"Total anomalies row in shopping cart columns:\", len(anom_shopping_cart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when using the same code, the anomalies is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle is_happy_customer\n",
    "\n",
    "based on the question, to check whether a customer is happy with their last order, the customer's latest review is\n",
    "classified using a sentiment analysis classifier. `SentimentIntensityAnalyzer` from\n",
    "`nltk.sentiment.vader` is used to obtain the polarity score. A sentiment is considered positive if\n",
    "it has a 'compound' polarity score of 0.05 or higher and is considered negative otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('vader_lexicon')\n",
    "# load the SentimentIntensityAnalyzer\n",
    "sentiment_analysis = SentimentIntensityAnalyzer()\n",
    "\n",
    "# use the code based on the question\n",
    "dirty_df[\"predict_customer_review\"] = dirty_df[\"latest_customer_review\"]\\\n",
    "    .apply(lambda x : True if sentiment_analysis\\\n",
    "           .polarity_scores(x)[\"compound\"]>0.05 or x == \"None\" else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_review = []\n",
    "\n",
    "# check the predicted value with the original columns\n",
    "for i in range(len(dirty_df)):\n",
    "    if dirty_df[\"predict_customer_review\"][i] != \\\n",
    "        dirty_df[\"is_happy_customer\"][i]:\n",
    "            anom_review.append(dirty_df[\"is_happy_customer\"][i])\n",
    "print(\"Total anomalies row in review columns:\", len(anom_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the value of the is_happy_customer\n",
    "dirty_df[\"is_happy_customer\"] = dirty_df[\"predict_customer_review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_review = []\n",
    "for i in range(len(dirty_df)):\n",
    "    if dirty_df[\"predict_customer_review\"][i] != \\\n",
    "        dirty_df[\"is_happy_customer\"][i]:\n",
    "            correct_review.append(dirty_df[\"is_happy_customer\"][i])\n",
    "print(\"incorrect review:\",len(correct_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handle Missing Data\n",
    "\n",
    "Even though `is_expedited_delivery` is still dirty, we will deal with the missing data. Missing data is a data that some value has a `Nan` value this anomalies can be called `Coverage Anomalies` which is decrease the amount of entities and entity properties from the mini-world that are represented in the data collection. Coverage anomalies are categorized as: missing values and missing tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output in above, there are 500 entries in the data. some columns has missing value, those columns are:\n",
    "\n",
    "* `nearest_warehouse`:             55 missing data, only have 445 entries out of 500\n",
    "\n",
    "* `order_price`:                   15 missing data, only have 485 entries out of 500\n",
    "\n",
    "* `delivery_charges`:              40 missing data, only have 460 entries out of 500\n",
    "            \n",
    "* `order_total`:                   15 missing data, only have 485 entries out of 500\n",
    "\n",
    "* `distance_to_nearest_warehouse`: 31 missing data, only have 469 entries out of 500\n",
    "\n",
    "* `is_happy_customer`:             40 missing data, only have 460 entries out of 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the missing index in every columns\n",
    "dist_warehouse_near_index_miss = list(missing_df[\n",
    "    missing_df[\"distance_to_nearest_warehouse\"].isnull()].index)\n",
    "nearest_warehouse_index_miss = list(missing_df[\n",
    "    missing_df[\"nearest_warehouse\"].isnull()].index)\n",
    "order_price_index_miss = list(missing_df[\n",
    "    missing_df[\"order_price\"].isnull()].index)\n",
    "delivery_charges_index_miss = list(missing_df[\n",
    "    missing_df[\"delivery_charges\"].isnull()].index)\n",
    "order_total_index_miss = list(missing_df[\n",
    "    missing_df[\"order_total\"].isnull()].index)\n",
    "is_happy_customer_index_miss = list(missing_df[\n",
    "    missing_df[\"is_happy_customer\"].isnull()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already solve the columns from the dirty data. we will use the the same code to get the value for the missing data. Firstly, we will start from the `distance_to_nearest_warehouse` in the columns we can use the function from the previous cells and use in the missing data and since there is no mistake/error on the missing data we can use other columns as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance in each warehouses and put into a new columns\n",
    "missing_df[\"distance_to_Nickolson\"] = missing_df.apply(\n",
    "    lambda x : dist_places_warehouse(x[\"customer_lat\"], \n",
    "                                     x[\"customer_long\"], \n",
    "                                    Nickolson_coor[\"lat\"],\n",
    "                                    Nickolson_coor[\"lon\"]),\n",
    "    axis=1).round(4)\n",
    "\n",
    "missing_df[\"distance_to_Thompson\"] = missing_df.apply(\n",
    "    lambda x : dist_places_warehouse(x[\"customer_lat\"],\n",
    "                                     x[\"customer_long\"], \n",
    "                                    Thompson_coor[\"lat\"],\n",
    "                                    Thompson_coor[\"lon\"]), \n",
    "    axis=1).round(4)\n",
    "\n",
    "missing_df[\"distance_to_Bakers\"] = missing_df.apply(\n",
    "    lambda x : dist_places_warehouse(x[\"customer_lat\"], \n",
    "                                     x[\"customer_long\"], \n",
    "                                    Bakers_coor[\"lat\"],\n",
    "                                    Bakers_coor[\"lon\"]), \n",
    "    axis=1).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a empty list\n",
    "min_nearest = []\n",
    "correct_warehouse = []\n",
    "warehouse_places = list(warehouses_df[\"names\"])\n",
    "\n",
    "# create a loop to find which column has the minimum value\n",
    "for h in range(len(missing_df)):\n",
    "    dist_near = round(missing_df[\"distance_to_nearest_warehouse\"][h], 4)\n",
    "    wr_place = missing_df[\"nearest_warehouse\"][h]\n",
    "    list_wr = [missing_df[\"distance_to_Nickolson\"][h],\n",
    "               missing_df[\"distance_to_Thompson\"][h],\n",
    "               missing_df[\"distance_to_Bakers\"][h]]\n",
    "    min_dist_idx = list_wr.index(min(list_wr))\n",
    "    correct_warehouse.append(warehouse_places[min_dist_idx])\n",
    "    min_nearest.append(min(list_wr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the value of the missing value\n",
    "list_miss_warehouse = [min_nearest[i] \\\n",
    "                       for i in dist_warehouse_near_index_miss]\n",
    "list_miss_distance = [correct_warehouse[i] \\\n",
    "                      for i in nearest_warehouse_index_miss]\n",
    "\n",
    "missing_df.loc[nearest_warehouse_index_miss,\n",
    "               \"nearest_warehouse\"] = list_miss_distance\n",
    "missing_df.loc[dist_warehouse_near_index_miss,\n",
    "               \"distance_to_nearest_warehouse\"] = list_miss_warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is for the happy customer, we will use the same build in function which is `SentimentIntensityAnalyzer()` to get the value of the customer happiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the sentiment anylyzer \n",
    "# from the package to sove the missing data in happy customer\n",
    "sentiment_analysis = SentimentIntensityAnalyzer()\n",
    "missing_df.loc[\n",
    "    is_happy_customer_index_miss, \"is_happy_customer\"] \\\n",
    "        = missing_df.loc[\n",
    "        is_happy_customer_index_miss,\n",
    "            \"latest_customer_review\"]\\\n",
    "    .apply(lambda x \\\n",
    "           : True if sentiment_analysis.polarity_scores(x)[\"compound\"]>0.05 \n",
    "           or x == \"None\" else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we done with the customer happiness we need to fill the `order_price` columns since we already have the price of each item from the previous cells. we will use this price item to calculate the missing price with the shopping cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the column based on the missing index\n",
    "missing_df.loc[order_price_index_miss][[\"shopping_cart\", \"order_price\"]]\n",
    "\n",
    "# create an array of the items quantity\n",
    "missing_df['item_list'] = missing_df.shopping_cart.str.replace(\"(\", \"[\")\\\n",
    "                            .str.replace(\")\", \"]\").apply(eval)\n",
    "missing_df['linalg_arr'] = missing_df.apply(pro_qty, axis=1)\n",
    "\n",
    "# calculate the order price value\n",
    "missing_df.loc[order_price_index_miss, \"order_price\"] = missing_df\\\n",
    "                .loc[order_price_index_miss].apply(dot_pro, axis=\"columns\")\n",
    "missing_df[\"order_price\"] = missing_df[\"order_price\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the `delivery_charges` based on the question, The store has different business rules depending on the season to match the different\n",
    "demands of each season. For example, delivery charge is calculated using a linear model\n",
    "which differs depending on the season. The model depends linearly (but in different ways for\n",
    "each season) on:\n",
    "1. Distance between customer and nearest warehouse\n",
    "2. Whether the customer wants an expedited delivery\n",
    "3. Whether the customer was happy with his/her last purchase (if no previous purchase,\n",
    "it is assumed that the customer is happy)\n",
    "\n",
    "since we already create the linear mode function from the begining using package called `sklearn` we will fit the model that have non-missing value and predict the missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the missing value and the non-missing value\n",
    "non_missing_delivery = missing_df[missing_df[\"delivery_charges\"].notnull()]\n",
    "missing_delivery = missing_df[missing_df[\"delivery_charges\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter each data based on the season and get the missing index\n",
    "winter_non_missing = non_missing_delivery[non_missing_delivery['season']\n",
    "                                          == \"Winter\"]\n",
    "winter_missing = missing_delivery[missing_delivery['season'] == \"Winter\"]\n",
    "winter_index_miss = list(winter_missing.index)\n",
    "\n",
    "summer_non_missing = non_missing_delivery[non_missing_delivery['season']\n",
    "                                          == \"Summer\"]\n",
    "summer_missing = missing_delivery[missing_delivery['season'] == \"Summer\"]\n",
    "summer_index_miss = list(summer_missing.index)\n",
    "\n",
    "autumn_non_missing = non_missing_delivery[non_missing_delivery['season']\n",
    "                                          == \"Autumn\"]\n",
    "autumn_missing = missing_delivery[missing_delivery['season'] == \"Autumn\"]\n",
    "autumn_index_miss = list(autumn_missing.index)\n",
    "\n",
    "spring_non_missing = non_missing_delivery[non_missing_delivery['season']\n",
    "                                          == \"Spring\"]\n",
    "\n",
    "spring_missing = missing_delivery[missing_delivery['season'] == \"Spring\"]\n",
    "spring_index_miss = list(spring_missing.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since there is only 460 correct data in the `delivery_charges` we will use them to fit the data and we will four different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the data for every season\n",
    "winter_model_missing = linearModel(winter_non_missing)\n",
    "summer_model_missing = linearModel(summer_non_missing)\n",
    "autumn_model_missing = linearModel(autumn_non_missing)\n",
    "spring_model_missing = linearModel(spring_non_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coeficcient\n",
    "print(winter_model_missing.coef_, winter_model_missing.intercept_)\n",
    "print(summer_model_missing.coef_, summer_model_missing.intercept_)\n",
    "print(autumn_model_missing.coef_, autumn_model_missing.intercept_)\n",
    "print(spring_model_missing.coef_, spring_model_missing.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Coefficient and intercept we will be used to predict the value for of the `delivery_charges`. this model we will be use in the outliers data and to clean the `is_expedited_delivery` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the rest of the data using the model based on the season\n",
    "predict_winter_miss = winter_model_missing.predict(winter_missing[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])\n",
    "predict_summer_miss = summer_model_missing.predict(summer_missing[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])\n",
    "predict_autumn_miss = autumn_model_missing.predict(autumn_missing[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])\n",
    "predict_spring_miss = spring_model_missing.predict(spring_missing[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])\n",
    "\n",
    "# round the result into 2 decimal\n",
    "missing_df.loc[winter_index_miss, \"delivery_charges\"] = predict_winter_miss\\\n",
    "            .round(2)\n",
    "missing_df.loc[summer_index_miss, \"delivery_charges\"] = predict_summer_miss\\\n",
    "            .round(2)\n",
    "missing_df.loc[autumn_index_miss, \"delivery_charges\"] = predict_autumn_miss\\\n",
    "            .round(2)\n",
    "missing_df.loc[spring_index_miss, \"delivery_charges\"] = predict_spring_miss\\\n",
    "            .round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we done with the missing `delivery_charges`, we will find the missing value for the total of the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the discount\n",
    "discount_miss = (missing_df.loc[order_total_index_miss,\"coupon_discount\"]/100)\n",
    "cal_discount_result = (missing_df.loc[order_total_index_miss,\"order_price\"] - \n",
    "                      (missing_df.loc[order_total_index_miss,\"order_price\"]\n",
    "                       *discount_miss))\n",
    "\n",
    "# get the delivery charges and calculate the missing total\n",
    "deliv_miss = (missing_df.loc[order_total_index_miss,\"delivery_charges\"])\n",
    "missing_df.loc[order_total_index_miss, \"order_total\"]= cal_discount_result \\\n",
    "                                                    + deliv_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_missing_df = missing_df[columns_dirty]\n",
    "clean_missing_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the missing data are all fill with the reasonable value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handle Outliers data\n",
    "\n",
    "In this section  we will be explore the outliers data and detect the outliers in the `delivery_charges` columns. we will use some visualization package such as `seaborn` and `matplotlib` to find the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column that contain zero\n",
    "outliers_df[\"predict_delivery\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data based on the season and get the index\n",
    "winter_outliers = outliers_df[outliers_df['season'] == \"Winter\"]\n",
    "winter_index = list(winter_outliers.index)\n",
    "\n",
    "summer_outliers = outliers_df[outliers_df['season'] == \"Summer\"]\n",
    "summer_index = list(summer_outliers.index)\n",
    "\n",
    "autumn_outliers = outliers_df[outliers_df['season'] == \"Autumn\"]\n",
    "autumn_index = list(autumn_outliers.index)\n",
    "\n",
    "spring_outliers = outliers_df[outliers_df['season'] == \"Spring\"]\n",
    "spring_index = list(spring_outliers.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the missing data model to predict the value \n",
    "missing_predict_winter = list(winter_model_missing.predict(winter_outliers[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']]))\n",
    "\n",
    "missing_predict_summer = list(summer_model_missing.predict(summer_outliers[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']]))\n",
    "\n",
    "missing_predict_autumn = list(autumn_model_missing.predict(autumn_outliers[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']]))\n",
    "\n",
    "missing_predict_spring = list(spring_model_missing.predict(spring_outliers[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the prediction value to the predict_delivery columns based in the index\n",
    "outliers_df.loc[winter_index, \"predict_delivery\"] = missing_predict_winter\n",
    "\n",
    "outliers_df.loc[summer_index, \"predict_delivery\"] = missing_predict_summer\n",
    "\n",
    "outliers_df.loc[autumn_index, \"predict_delivery\"] = missing_predict_autumn\n",
    "\n",
    "outliers_df.loc[spring_index, \"predict_delivery\"] = missing_predict_spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new columns to monitor the error\n",
    "outliers_df[\"residual\"] = (outliers_df[\"predict_delivery\"] - \n",
    "                           outliers_df[\"delivery_charges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the delivery charges columns for each season\n",
    "sns.boxplot(x=\"season\", y=\"delivery_charges\",\n",
    "            data=outliers_df)\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the box plot, there are some value that out of the box. this value will be detect and delete from the data. to do that we will plot the error which the `residual` columns and the error value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(outliers_df.loc[winter_index][\"residual\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(outliers_df.loc[summer_index][\"residual\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(outliers_df.loc[autumn_index][\"residual\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(outliers_df.loc[spring_index][\"residual\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the plot of the error in each season. we can see there are some large error value. This value is from the difference between the prediction and the original. Therfore we will delete the row that have the outliers using the function the we build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/a-short-journey-\n",
    "# of-outlier-detection-bdf143464a92\n",
    "def detect_outliers_index(columns):\n",
    "    # calculate the first quantile and third  quantile\n",
    "    Q1 = columns[\"residual\"].quantile(0.25)\n",
    "    Q3 = columns[\"residual\"].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    bool_outliers = (columns[\"residual\"] < (Q1 - 1.5 * IQR)) \\\n",
    "        | (columns[\"residual\"] > (Q3 + 1.5 * IQR))\n",
    "    return list(bool_outliers[bool_outliers == True].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function for detect the outliers index\n",
    "outliers_index_winter = detect_outliers_index(outliers_df.loc[winter_index])\n",
    "outliers_index_summer = detect_outliers_index(outliers_df.loc[summer_index])\n",
    "outliers_index_autumn = detect_outliers_index(outliers_df.loc[autumn_index])\n",
    "outliers_index_spring = detect_outliers_index(outliers_df.loc[spring_index])\n",
    "\n",
    "# total the index\n",
    "index_list_outliers = outliers_index_winter \\\n",
    "    + outliers_index_summer + outliers_index_autumn \\\n",
    "        + outliers_index_spring\n",
    "\n",
    "print(\"outliers detect:\", len(index_list_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the row based on the index\n",
    "new_outliers = outliers_df.drop(index_list_outliers).reset_index(drop=True)\n",
    "new_outliers = new_outliers[columns_dirty]\n",
    "new_outliers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"season\", y=\"delivery_charges\",\n",
    "            data=new_outliers)\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detect 50 outliers in the data and delete them, then we plot the season and delivery charged again to see if there is another outliers. The data is turn from 500 rows to 450 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handle Dirty Is expedited delivery columns\n",
    "\n",
    "Last problem that need to be clean in the dirty data is the `is_expedited_delivery` columns. the process to solve this problem is:\n",
    "\n",
    "* Filter the data based on the season and get the index\n",
    "* use the linear model to predict the value\n",
    "* use histogram to see the error value and see how many row that have large value error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_df[\"is_expedited_delivery\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiter the data based on the season and index\n",
    "winter_dirty = dirty_df[dirty_df[\"season\"] == \"Winter\"][[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer', \"delivery_charges\"]]\n",
    "winter_dirty_index = list(winter_dirty.index)\n",
    "\n",
    "summer_dirty = dirty_df[dirty_df[\"season\"] == \"Summer\"][[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer', \"delivery_charges\"]]\n",
    "summer_dirty_index = list(summer_dirty.index)\n",
    "\n",
    "autumn_dirty = dirty_df[dirty_df[\"season\"] == \"Autumn\"][[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer', \"delivery_charges\"]]\n",
    "autumn_dirty_index = list(autumn_dirty.index)\n",
    "\n",
    "spring_dirty = dirty_df[dirty_df[\"season\"] == \"Spring\"][[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer', \"delivery_charges\"]]\n",
    "spring_dirty_index = list(spring_dirty.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the delivery charge\n",
    "winter_predict_dirty = winter_model_missing.predict(winter_dirty[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])\n",
    "\n",
    "summer_predict_dirty = summer_model_missing.predict(summer_dirty[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])\n",
    "\n",
    "autumn_predict_dirty = autumn_model_missing.predict(autumn_dirty[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])\n",
    "\n",
    "spring_predict_dirty = spring_model_missing.predict(spring_dirty[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the error and get the data that have more than 5 and less tha -5\n",
    "winter_error_dirty = winter_dirty[\"delivery_charges\"] - winter_predict_dirty\n",
    "plt.hist((winter_error_dirty))\n",
    "plt.show()\n",
    "index_winter_anom = list(winter_error_dirty[(winter_error_dirty > 5) \\\n",
    "                                            | (winter_error_dirty < -5)].index)\n",
    "\n",
    "summer_error_dirty = summer_dirty[\"delivery_charges\"] - summer_predict_dirty\n",
    "plt.hist((summer_error_dirty))\n",
    "plt.show()\n",
    "index_summer_anom = list(summer_error_dirty[(summer_error_dirty > 5) \\\n",
    "                                            | (summer_error_dirty < -5)].index)\n",
    "\n",
    "autumn_error_dirty = autumn_dirty[\"delivery_charges\"] - autumn_predict_dirty\n",
    "plt.hist((autumn_error_dirty))\n",
    "plt.show()\n",
    "index_autumn_anom = list(autumn_error_dirty[(autumn_error_dirty > 5) \\\n",
    "                                            | (autumn_error_dirty < -5)].index)\n",
    "\n",
    "spring_error_dirty = spring_dirty[\"delivery_charges\"] - spring_predict_dirty\n",
    "plt.hist((spring_error_dirty))\n",
    "plt.show()\n",
    "index_spring_anom = list(spring_error_dirty[(spring_error_dirty > 5) \\\n",
    "                                            | (spring_error_dirty < -5)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the histogram, there are some high error. We will use -5 < error < 5 as a boundary to change the `is_expedited_delivery` where the value is outside of that boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_expe_deliv = len(index_spring_anom) + len(index_autumn_anom) \\\n",
    "                    + len(index_summer_anom) + len(index_winter_anom)\n",
    "print(\"Total anomalies for is expedited delivery: \", anom_expe_deliv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to change the value\n",
    "def expi_deliv(columns):\n",
    "    if columns[\"is_expedited_delivery\"] == True:\n",
    "        columns[\"is_expedited_delivery\"] = False\n",
    "    else:\n",
    "        columns[\"is_expedited_delivery\"] = True\n",
    "        \n",
    "    return columns[\"is_expedited_delivery\"]\n",
    "\n",
    "# apply the fucntion and put it depend on the index\n",
    "dirty_df.loc[index_winter_anom, \n",
    "             \"is_expedited_delivery\"] = dirty_df.loc[index_winter_anom]\\\n",
    "                                            .apply(expi_deliv, axis = 1)\n",
    "dirty_df.loc[index_summer_anom, \n",
    "             \"is_expedited_delivery\"] = dirty_df.loc[index_summer_anom]\\\n",
    "                                            .apply(expi_deliv, axis = 1)\n",
    "dirty_df.loc[index_autumn_anom, \n",
    "             \"is_expedited_delivery\"] = dirty_df.loc[index_autumn_anom]\\\n",
    "                                            .apply(expi_deliv, axis = 1)\n",
    "dirty_df.loc[index_spring_anom, \n",
    "             \"is_expedited_delivery\"] = dirty_df.loc[index_spring_anom]\\\n",
    "                                            .apply(expi_deliv, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiter the data based on the season and index\n",
    "winter_dirty = dirty_df[dirty_df[\"season\"] == \"Winter\"][[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer', \"delivery_charges\"]]\n",
    "winter_dirty_index = list(winter_dirty.index)\n",
    "\n",
    "summer_dirty = dirty_df[dirty_df[\"season\"] == \"Summer\"][[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer', \"delivery_charges\"]]\n",
    "summer_dirty_index = list(summer_dirty.index)\n",
    "\n",
    "autumn_dirty = dirty_df[dirty_df[\"season\"] == \"Autumn\"][[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer', \"delivery_charges\"]]\n",
    "autumn_dirty_index = list(autumn_dirty.index)\n",
    "\n",
    "spring_dirty = dirty_df[dirty_df[\"season\"] == \"Spring\"][[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer', \"delivery_charges\"]]\n",
    "spring_dirty_index = list(spring_dirty.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the delivery charge\n",
    "winter_predict_dirty = winter_model_missing.predict(winter_dirty[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])\n",
    "\n",
    "summer_predict_dirty = summer_model_missing.predict(summer_dirty[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])\n",
    "\n",
    "autumn_predict_dirty = autumn_model_missing.predict(autumn_dirty[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])\n",
    "\n",
    "spring_predict_dirty = spring_model_missing.predict(spring_dirty[[\n",
    "    'distance_to_nearest_warehouse',\n",
    "    'is_expedited_delivery',\n",
    "    'is_happy_customer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the error and get the data that have more than 10 and less tha -10\n",
    "winter_error_dirty = winter_dirty[\"delivery_charges\"] - winter_predict_dirty\n",
    "plt.hist((winter_error_dirty))\n",
    "plt.show()\n",
    "index_winter_new = list(winter_error_dirty[(winter_error_dirty > 5) \n",
    "                                           | (winter_error_dirty < -5)].index)\n",
    "\n",
    "summer_error_dirty = summer_dirty[\"delivery_charges\"] - summer_predict_dirty\n",
    "plt.hist((summer_error_dirty))\n",
    "plt.show()\n",
    "index_summer_new = list(summer_error_dirty[(summer_error_dirty > 5) \n",
    "                                           | (summer_error_dirty < -5)].index)\n",
    "\n",
    "autumn_error_dirty = autumn_dirty[\"delivery_charges\"] - autumn_predict_dirty\n",
    "plt.hist((autumn_error_dirty))\n",
    "plt.show()\n",
    "index_autumn_new = list(autumn_error_dirty[(autumn_error_dirty > 5) \n",
    "                                           | (autumn_error_dirty < -5)].index)\n",
    "\n",
    "spring_error_dirty = spring_dirty[\"delivery_charges\"] - spring_predict_dirty\n",
    "plt.hist((spring_error_dirty))\n",
    "plt.show()\n",
    "index_spring_new = list(spring_error_dirty[(spring_error_dirty > 5) \n",
    "                                           | (spring_error_dirty < -5)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_df[\"is_expedited_delivery\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `is_expedited_delivery` is already clean and ready to be export."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Data\n",
    "\n",
    "We done with clean all the anomalies in the data, we just need to export the data into a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = dirty_df[columns_dirty]\n",
    "clean_df.info()\n",
    "\n",
    "clean_df.to_csv(\"30399262_dirty_data_solution.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_missing_df.info()\n",
    "\n",
    "clean_missing_df.to_csv(\"30399262_missing_data_solution.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outliers.info()\n",
    "new_outliers.to_csv(\"30399262_outlier_data_solution.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_anom_row = anom_date + len(anom_dist_wr) +\\\n",
    "    anom_expe_deliv +len(anom_coor)+ len(anom_order_price)+\\\n",
    "    len(anom_order_total)+\\\n",
    "    len(anom_review)+len(anom_season)+len(anom_shopping_cart)+len(anom_wr)\n",
    "print(\"Total Anomalies Row:\", total_anom_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "This assignment involves the knowledge on exploratory data analysis, outliers detection and removal, anomalies detection and correction, and data imputation techniques in Python programming language using libraries such as sklearn, re, numpy, seaborn, nltk, pandas, ittertools. The steps encompasses the following processes :\n",
    "\n",
    "\n",
    "- **Detect and clean dirty data**. By using pandas dataframe and seaborn to do exploratory data analysis, ntlk to clean the review and use np.linalg to get the each item price then find the anomalies in the shopping cart and order total. Then use visualization to get the coordinate and calculate the distance. lastly, use sklearn to perform the linear model in the is expedited delivery.\n",
    "\n",
    "- **Detect and impute missing values**. using the same process like clean the data we manange to fill the missing value in every columns that still have nan value\n",
    "\n",
    "- **Outliers detection and removal**. By using basic pandas dataframe manipulation and seaborn to generate boxplot.\n",
    "- **Observe the correct data in outlier data**. By numpy to calculate linear calculation, sklearn to generate linear model from other table to get the prediction, and pandas dataframe manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- Geeksforgeeks. (2020, April 20). *Python program to swap two elements in a list* Retrieved from https://www.geeksforgeeks.org/python-program-to-swap-two-elements-in-a-list/\n",
    "\n",
    "- Australian Government (2020, October 14). *Climate Glossary* Retrieved from http://www.bom.gov.au/climate/glossary/seasons.shtml\n",
    "\n",
    "- smooth007. (2015, October 9). *Using Pandas to calculate distance between coordinates from imported csv* Retrieved from https://stackoverflow.com/questions/33029396/using-pandas-to-calculate-distance-between-coordinates-from-imported-csv\n",
    "\n",
    "- Ryota Bannai (2019, January 13). *A short journey of outlier detection* Retrieved from https://towardsdatascience.com/a-short-journey-of-outlier-detection-bdf143464a92\n",
    "\n",
    "- Numpy (2020, October 14). *numpy.linalg.solve* Retrieved from https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html\n",
    "\n",
    "- NLTK 3.5 documentation (2020, October 14). *nltk.sentiment.sentiment_analyzer module* Retrieved from https://www.nltk.org/api/nltk.sentiment.html#nltk.sentiment.vader.SentimentIntensityAnalyzer.score_valence\n",
    "\n",
    "- Romaine Herrera (2014, February 27). *Regex date validation for yyyy-mm-dd [duplicate]* Retrieved from https://stackoverflow.com/questions/22061723/regex-date-validation-for-yyyy-mm-dd\n",
    "\n",
    "- Python document (2020, October 14) *itertools — Functions creating iterators for efficient looping* Retrieved from https://docs.python.org/2/library/itertools.html\n",
    "\n",
    "- user1654183 (2013, May 2) *Convert a Pandas DataFrame to a dictionary* Retrieved from https://stackoverflow.com/questions/16343752/numpy-where-function-multiple-conditions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
